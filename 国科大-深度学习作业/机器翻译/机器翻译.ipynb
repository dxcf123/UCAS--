{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6016f24",
   "metadata": {},
   "source": [
    "## 1 导入相关包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af03343c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import math\n",
    "import torch\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import copy\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import sacrebleu\n",
    "import random\n",
    "import time\n",
    "import jieba\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # 下面老是报错 shape 不一致"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f5dbfd",
   "metadata": {},
   "source": [
    "## 2 定义dataset类\n",
    "用于传入dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d07e8aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    # 创建数据集\n",
    "    def __init__(self, src, tgt):\n",
    "        \"\"\"\n",
    "        初始化\n",
    "        :param src: 源数据(经tokenizer处理后)\n",
    "        :param tgt: 目标数据(经tokenizer处理后)\n",
    "        \"\"\"\n",
    "        self.src = src\n",
    "        self.tgt = tgt\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.src[i], self.tgt[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dab7e16",
   "metadata": {},
   "source": [
    "## 3 定义tokenizer类\n",
    "其作用是读取源数据并将其处理成可供模型输入的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6744d89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer():\n",
    "    ## 定义tokenizer,对原始数据进行处理\n",
    "    def __init__(self, en_path, ch_path, count_min=5):\n",
    "        \"\"\"\n",
    "        初始化\n",
    "        :param en_path: 英文数据路径\n",
    "        :param ch_path: 中文数据路径\n",
    "        :param count_min: 对出现次数少于这个次数的数据进行过滤\n",
    "        \"\"\"\n",
    "        self.en_path = en_path  # 英文路径\n",
    "        self.ch_path = ch_path  # 中文路径\n",
    "        self.__count_min = count_min  # 对出现次数少于这个次数的数据进行过滤\n",
    "\n",
    "        # 读取原始英文数据\n",
    "        self.en_data = self.__read_ori_data(en_path)\n",
    "        # 读取原始中文数据\n",
    "        self.ch_data = self.__read_ori_data(ch_path)\n",
    "\n",
    "        self.index_2_word = ['unK', '<pad>', '<bos>', '<eos>']\n",
    "        self.word_2_index = {'unK': 0, '<pad>': 1, '<bos>': 2, '<eos>': 3}\n",
    "\n",
    "        self.en_set = set()\n",
    "        self.en_count = {}\n",
    "\n",
    "        # 中英文字符计数\n",
    "        self.__en_count = {}\n",
    "        self.__ch_count = {}\n",
    "\n",
    "        self.__count_word()\n",
    "        self.mx_length = 40\n",
    "        # 创建英文词汇表\n",
    "        self.data_ = []\n",
    "        self.__filter_data()\n",
    "        random.shuffle(self.data_)\n",
    "        self.test = self.data_[-1000:]\n",
    "        self.data_ = self.data_[:-1000]\n",
    "\n",
    "    def __read_ori_data(self, path):\n",
    "        \"\"\"\n",
    "        读取原始数据\n",
    "        :param path: 数据路径\n",
    "        :return: 返回一个列表，每个元素是一条数据\n",
    "        \"\"\"\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            data = f.read().split('\\n')[:-1]\n",
    "        return data\n",
    "\n",
    "    def __count_word(self):\n",
    "        \"\"\"\n",
    "        统计中英文词汇表\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        le = len(self.en_data)\n",
    "        p = 0\n",
    "        # 统计英文词汇表\n",
    "        for data in self.en_data:\n",
    "            if p % 1000 == 0:\n",
    "                print('英文', p / le)\n",
    "            sentence = word_tokenize(data)\n",
    "            for sen in sentence:\n",
    "#                 sen=sen.lower()\n",
    "                if sen in self.en_set:\n",
    "                    self.en_count[sen] += 1\n",
    "                else:\n",
    "                    self.en_set.add(sen)\n",
    "                    self.en_count[sen] = 1\n",
    "            p += 1\n",
    "        for k, v in self.en_count.items():\n",
    "            if v >= self.__count_min:\n",
    "                self.word_2_index[k] = len(self.index_2_word)\n",
    "                self.index_2_word.append(k)\n",
    "            else:\n",
    "                self.word_2_index[k] = 0\n",
    "        self.en_set = set()\n",
    "        self.en_count = {}\n",
    "        p = 0\n",
    "        # 统计中文词汇表\n",
    "        for data in self.ch_data:\n",
    "            if p % 1000 == 0:\n",
    "                print('中文', p / le)\n",
    "            sentence = list(jieba.cut(data))\n",
    "            for sen in sentence:\n",
    "                if sen in self.en_set:\n",
    "                    self.en_count[sen] += 1\n",
    "                else:\n",
    "                    self.en_set.add(sen)\n",
    "                    self.en_count[sen] = 1\n",
    "            p += 1\n",
    "        # 构建词汇表\n",
    "        for k, v in self.en_count.items():\n",
    "            if v >= self.__count_min:\n",
    "                self.word_2_index[k] = len(self.index_2_word)\n",
    "                self.index_2_word.append(k)\n",
    "            else:\n",
    "                self.word_2_index[k] = 0\n",
    "\n",
    "    def __filter_data(self):\n",
    "        length = len(self.en_data)\n",
    "        for i in range(length):\n",
    "            # 0 代表英文到中文，1 代表中文到英文\n",
    "            self.data_.append([self.en_data[i], self.ch_data[i], 0])\n",
    "            self.data_.append([self.ch_data[i], self.en_data[i], 1])\n",
    "\n",
    "    def en_cut(self, data):\n",
    "        data = word_tokenize(data)\n",
    "        # 用于存放每个句子对应的编码\n",
    "        if len(data) > self.mx_length:\n",
    "            return 0, []\n",
    "        en_tokens = []\n",
    "        # 对分词结果进行遍历\n",
    "        for tk in data:\n",
    "#             x = tk.lower()\n",
    "            # 对于结果进行编码,0代表unK\n",
    "            en_tokens.append(self.word_2_index.get(tk, 0))\n",
    "        return 1, en_tokens\n",
    "\n",
    "    def ch_cut(self, data):\n",
    "        data = list(jieba.cut(data))\n",
    "#         list(data)[:-1]\n",
    "        # 用于存放每个句子对应的编码\n",
    "        if len(data) > self.mx_length:\n",
    "            return 0, []\n",
    "        en_tokens = []\n",
    "        # 对分词结果进行遍历\n",
    "        for tk in data:\n",
    "            # 对于结果进行编码,0代表unK\n",
    "            en_tokens.append(self.word_2_index.get(tk, 0))\n",
    "        return 1, en_tokens\n",
    "\n",
    "    def encode_all(self, data):\n",
    "        \"\"\"\n",
    "        对一组数据进行编码\n",
    "        :param data: data是一个数组，形状为n*3 每个元素是[src_sentence, tgt_sentence, label]，label 0 代表英文到中文，1 代表中文到英文\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        src = []\n",
    "        tgt = []\n",
    "        en_src, en_tgt, l = [], [], []\n",
    "        labels=[]\n",
    "        for i in data:\n",
    "            en_src.append(i[0])\n",
    "            en_tgt.append(i[1])\n",
    "            l.append(i[2])\n",
    "        for i in range(len(l)):\n",
    "            if l[i] == 0:\n",
    "                lab1, src_tokens = self.en_cut(en_src[i])\n",
    "                if lab1 == 0:\n",
    "                    continue\n",
    "                lab2, tgt_tokens = self.ch_cut(en_tgt[i])\n",
    "                if lab2 == 0:\n",
    "                    continue\n",
    "                src.append(src_tokens)\n",
    "                tgt.append(tgt_tokens)\n",
    "                labels.append(i)\n",
    "            else:\n",
    "                lab1, tgt_tokens = self.en_cut(en_tgt[i])\n",
    "                if lab1 == 0:\n",
    "                    continue\n",
    "                lab2, src_tokens = self.ch_cut(en_src[i])\n",
    "                if lab2 == 0:\n",
    "                    continue\n",
    "                src.append(src_tokens)\n",
    "                tgt.append(tgt_tokens)\n",
    "                labels.append(i)\n",
    "        return labels,src, tgt\n",
    "\n",
    "    def encode(self, src, l):\n",
    "        if l == 0:\n",
    "            src1 = word_tokenize(src)\n",
    "            # 用于存放每个句子对应的编码\n",
    "            en_tokens = []\n",
    "            # 对分词结果进行遍历\n",
    "            for tk in src1:\n",
    "#                 x = tk.lower()\n",
    "                # 对于结果进行编码\n",
    "                en_tokens.append(self.word_2_index.get(tk, 0))\n",
    "            return [en_tokens]\n",
    "        else:\n",
    "            src1 = list(jieba.cut(src))\n",
    "            # 用于存放每个句子对应的编码\n",
    "            en_tokens = []\n",
    "            # 对分词结果进行遍历\n",
    "            for tk in src1:\n",
    "                # 对于结果进行编码\n",
    "                en_tokens.append(self.word_2_index.get(tk, 0))\n",
    "            return [en_tokens]\n",
    "\n",
    "    def decode(self, data):\n",
    "        \"\"\"\n",
    "        数据解码\n",
    "        :param data: 这里传入一个中文的index\n",
    "        :return: 返回解码后的一个字符\n",
    "        \"\"\"\n",
    "        return self.index_2_word[data]\n",
    "\n",
    "    def __get_datasets(self, data):\n",
    "        \"\"\"\n",
    "        获取数据集\n",
    "        :return:返回DataSet类型的数据 或者 None\n",
    "        \"\"\"\n",
    "        # 将数据编码并\n",
    "        labels,src, tgt = self.encode_all(data)\n",
    "        # 返回数据集\n",
    "        return TranslationDataset(src, tgt)\n",
    "\n",
    "    def another_process(self, batch_datas):\n",
    "        \"\"\"\n",
    "        特殊处理，这里传入一个batch的数据，并对这个batch的数据进行填充，使得每一行的数据长度相同。这里填充pad 空字符  bos 开始  eos结束\n",
    "        :param batch_datas: 一个batch的数据\n",
    "        :return: 返回填充后的数据\n",
    "        \"\"\"\n",
    "        # 创建四个空字典存储数据\n",
    "        en_index, ch_index = [], []  # 中文英文索引，中文索引\n",
    "        en_len, ch_len = [], []  # 没行英文长度，每行中文长度\n",
    "\n",
    "        for en, ch in batch_datas:  # 对batch进行遍历，将所有数据的索引与长度加入四个列表\n",
    "            en_index.append(en)\n",
    "            ch_index.append(ch)\n",
    "            en_len.append(len(en))\n",
    "            ch_len.append(len(ch))\n",
    "\n",
    "        # 获取中英文的最大长度，根据这个长度对所有数据进行填充，使每行数据长度相同\n",
    "        max_en_len = max(en_len)\n",
    "        max_ch_len = max(ch_len)\n",
    "        max_len = max(max_en_len, max_ch_len + 2)\n",
    "\n",
    "        # 英文数据填充，i是原始数据，后面是填充的pad\n",
    "        en_index = [i + [self.word_2_index['<pad>']] * (max_len - len(i)) for i in en_index]\n",
    "        # 中文数据填充 先填充bos表示句子开始，后面接原始数据，最后填充eos表示句子结束，后面接pad\n",
    "        ch_index = [[self.word_2_index['<bos>']] + i + [self.word_2_index['<eos>']] +\n",
    "                    [self.word_2_index['<pad>']] * (max_len - len(i) + 1) for i in ch_index]\n",
    "\n",
    "        # 将处理后的数据转换为tensor并放到相应设备上\n",
    "        en_index = torch.tensor(en_index)\n",
    "        ch_index = torch.tensor(ch_index)\n",
    "        return en_index, ch_index\n",
    "\n",
    "    def get_dataloader(self, data, batch_size=40):\n",
    "        \"\"\"\n",
    "        获取dataloader\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # 获取数据集\n",
    "        data = self.__get_datasets(data)\n",
    "        # 返回DataLoader类型的数据\n",
    "        return DataLoader(data, batch_size=batch_size, shuffle=True, collate_fn=self.another_process)\n",
    "\n",
    "    # 获取英文词表大小\n",
    "    def get_vocab_size(self):\n",
    "        return len(self.index_2_word)\n",
    "\n",
    "    # 获取数据集大小\n",
    "    def get_dataset_size(self):\n",
    "        return len(self.en_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f6d085",
   "metadata": {},
   "source": [
    "## 4 定义batch类\n",
    "其作用是生成掩码与统计非填充字符数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f84ee93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    # 批次类,对每一个批次的数据进行掩码生成操作\n",
    "    def __init__(self, src, trg=None, tokenizer=None, device='cuda'):\n",
    "        \"\"\"\n",
    "        初始化函数\n",
    "        :param src: 源数据\n",
    "        :param trg: 目标数据\n",
    "        :param tokenizer: 分词器\n",
    "        :param device: 训练设备\n",
    "        \"\"\"\n",
    "        # 将输入、输出单词id表示的数据规范成整数类型并转换到训练设备上\n",
    "        src = src.to(device).long()\n",
    "        trg = trg.to(device).long()\n",
    "        self.src = src  # 源数据 (batch, seq_len)\n",
    "        self.__pad = tokenizer.word_2_index['<pad>']  # 填充字符的索引\n",
    "        # 对于当前输入的语句非空部分进行判断，这里是对源数据进行掩码操作，将填充的内容置为0\n",
    "        # 并在seq length前面增加一维，形成维度为 1×seq length 的矩阵\n",
    "        self.src_mask = (src != self.__pad).unsqueeze(-2)\n",
    "        # 如果输出目标不为空，则需要对解码器使用的目标语句进行掩码\n",
    "        if trg is not None:\n",
    "            # 解码器使用的目标输入部分\n",
    "            self.trg = trg[:, : -1]\n",
    "            # 解码器训练时应预测输出的目标结果\n",
    "            self.trg_y = trg[:, 1:]\n",
    "            # 将目标输入部分进行注意力掩码\n",
    "            self.trg_mask = self.make_std_mask(self.trg, self.__pad)\n",
    "            # 将应输出的目标结果中实际的词数进行统计\n",
    "            self.ntokens = (self.trg_y != self.__pad).data.sum()\n",
    "\n",
    "    # 掩码操作\n",
    "    @staticmethod\n",
    "    def make_std_mask(tgt, pad):\n",
    "        \"\"\"\n",
    "        生成掩码矩阵\n",
    "        :param tgt: 目标数据\n",
    "        :param pad: 填充字符的索引\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)  # 首先对pad进行掩码生成\n",
    "        # 这里对注意力进行掩码操作并与pad掩码结合起来。\n",
    "        tgt_mask = tgt_mask & Variable(subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
    "        return tgt_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4b2266",
   "metadata": {},
   "source": [
    "#### 注意力掩码生成函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "26bfeea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"\"\"\n",
    "    注意力机制掩码生成\n",
    "    :param size: 句子长度\n",
    "    :return: 注意力掩码\n",
    "    \"\"\"\n",
    "    # 设定subsequent_mask矩阵的shape\n",
    "    attn_shape = (1, size, size)\n",
    "    # 生成一个右上角(不含主对角线)为全1，左下角(含主对角线)为全0的subsequent_mask矩阵\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    # 返回一个右上角(不含主对角线)为全False，左下角(含主对角线)为全True的subsequent_mask矩阵\n",
    "    return torch.from_numpy(subsequent_mask) == 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97df4f92",
   "metadata": {},
   "source": [
    "## 5 词嵌入类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bcaf8597",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    # 词嵌入层\n",
    "    def __init__(self, d_model, vocab):\n",
    "        \"\"\"\n",
    "        词嵌入层初始化\n",
    "        :param d_model: 词嵌入维度\n",
    "        :param vocab: 词表大小\n",
    "        \"\"\"\n",
    "        super(Embedding, self).__init__()\n",
    "        # Embedding层\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        # Embedding维数\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 返回x的词向量（需要乘以math.sqrt(d_model)）\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fccb95",
   "metadata": {},
   "source": [
    "## 6 位置编码器类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b0c071d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    # 位置编码器层\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000, device='cuda'):\n",
    "        \"\"\"\n",
    "        位置编码器层初始化\n",
    "        :param d_model: 词嵌入维度\n",
    "        :param dropout: dropout比例\n",
    "        :param max_len: 序列最大长度\n",
    "        :param device: 训练设备\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        # 位置编码矩阵，维度[max_len, embedding_dim]\n",
    "        pe = torch.zeros(max_len, d_model, device=device)\n",
    "        # 单词位置\n",
    "        position = torch.arange(0.0, max_len, device=device)\n",
    "        position.unsqueeze_(1)\n",
    "        # 使用exp和log实现幂运算\n",
    "        div_term = torch.exp(torch.arange(0.0, d_model, 2, device=device) * (- math.log(1e4) / d_model))\n",
    "        div_term.unsqueeze_(0)\n",
    "        # 计算单词位置沿词向量维度的纹理值\n",
    "        pe[:, 0:: 2] = torch.sin(torch.mm(position, div_term))\n",
    "        pe[:, 1:: 2] = torch.cos(torch.mm(position, div_term))\n",
    "        # 增加批次维度，[1, max_len, embedding_dim]\n",
    "        pe.unsqueeze_(0)\n",
    "        # 将位置编码矩阵注册为buffer(不参加训练)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 将一个批次中语句所有词向量与位置编码相加\n",
    "        # 注意，位置编码不参与训练，因此设置requires_grad=False\n",
    "        x += Variable(self.pe[:, : x.size(1), :], requires_grad=False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82d6f3a",
   "metadata": {},
   "source": [
    "## 7 多头注意力机制类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5d0349f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    # 多头注意力机制\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"\"\"\n",
    "        多头注意力机制初始化\n",
    "        :param h: 多头\n",
    "        :param d_model: 词嵌入维度\n",
    "        :param dropout: dropout比例\n",
    "        \"\"\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        # 确保整除\n",
    "        assert d_model % h == 0\n",
    "        # q、k、v向量维数\n",
    "        self.d_k = d_model // h\n",
    "        # 头的数量\n",
    "        self.h = h\n",
    "        # WQ、WK、WV矩阵及多头注意力拼接变换矩阵WO 4个线性层\n",
    "        self.linears = nn.ModuleList([copy.deepcopy(nn.Linear(d_model, d_model)) for _ in range(4)])\n",
    "        # 注意力机制函数\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        反向传播\n",
    "        :param query: q\n",
    "        :param key: k\n",
    "        :param value: v\n",
    "        :param mask: 掩码\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "        # 批次大小\n",
    "        nbatches = query.size(0)\n",
    "        # WQ、WK、WV分别对词向量线性变换，并将结果拆成h块\n",
    "        query, key, value = [\n",
    "            l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "            for l, x in zip(self.linears, (query, key, value))\n",
    "        ]\n",
    "        # 注意力加权\n",
    "        x, self.attn = self.attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "        # 多头注意力加权拼接\n",
    "        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)\n",
    "        # 对多头注意力加权拼接结果线性变换\n",
    "        return self.linears[-1](x)\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask=None, dropout=None):\n",
    "        \"\"\"\n",
    "        注意力加权\n",
    "        :param query: q\n",
    "        :param key: k\n",
    "        :param value: v\n",
    "        :param mask: 掩码矩阵\n",
    "        :param dropout: dropout比例\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # q、k、v向量长度为d_k\n",
    "        d_k = query.size(-1)\n",
    "        # 矩阵乘法实现q、k点积注意力，sqrt(d_k)归一化\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        # 注意力掩码机制\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        # 注意力矩阵softmax归一化\n",
    "        p_attn = F.softmax(scores, dim=-1)\n",
    "        # dropout\n",
    "        if dropout is not None:\n",
    "            p_attn = dropout(p_attn)\n",
    "        # 注意力对v加权\n",
    "        return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929c47ed",
   "metadata": {},
   "source": [
    "## 8 子层连接结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6de60f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    # 子层连接结构 用于连接注意力机制以及前馈全连接网络\n",
    "    def __init__(self, d_model, dropout):\n",
    "        \"\"\"\n",
    "        子层连接结构初始化层\n",
    "        :param d_model: 词嵌入纬度\n",
    "        :param dropout: dropout比例\n",
    "        \"\"\"\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        # 规范化层\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        # dropout层\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        # 层归一化\n",
    "        x_ = self.norm(x)\n",
    "        x_ = sublayer(x_)\n",
    "        x_ = self.dropout(x_)\n",
    "        # 残差连接\n",
    "        return x + x_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf100c0",
   "metadata": {},
   "source": [
    "## 9 前馈全连接网络类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d5a692cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    # 前馈全连接网络\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        前馈全连接网络初始化层\n",
    "        :param d_model: 词嵌入维度\n",
    "        :param d_ff: 中间隐层维度\n",
    "        :param dropout: dropout比例\n",
    "        \"\"\"\n",
    "        super(FeedForward, self).__init__()\n",
    "        # 全连接层\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        # Dropout层\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.w_1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.w_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b592b9ca",
   "metadata": {},
   "source": [
    "## 10 编码器类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "acb56fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    # 编码器\n",
    "    def __init__(self, h, d_model, d_ff=256, dropout=0.1):\n",
    "        \"\"\"\n",
    "        编码器层初始化\n",
    "        :param h: 头数\n",
    "        :param d_model: 词嵌入维度\n",
    "        :param d_ff: 中间隐层维度\n",
    "        :param dropout: dropout比例\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        # 多头注意力\n",
    "        self.self_attn = MultiHeadedAttention(h, d_model)\n",
    "        # 前馈全连接层\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        # 子层连接结构\n",
    "        self.sublayer1 = SublayerConnection(d_model, dropout)\n",
    "        self.sublayer2 = SublayerConnection(d_model, dropout)\n",
    "        # 规范化层\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # 将embedding层进行Multi head Attention\n",
    "        x = self.sublayer1(x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        # attn的结果直接作为下一层输入\n",
    "        return self.norm(self.sublayer2(x, self.feed_forward))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82943b33",
   "metadata": {},
   "source": [
    "## 11 解码器类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "facdc3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, h, d_model, d_ff=256, dropout=0.1):\n",
    "        \"\"\"\n",
    "        解码器层\n",
    "        :param h: 头数\n",
    "        :param d_model: 词嵌入维度\n",
    "        :param d_ff: 中间隐层维度\n",
    "        :param dropout: dropout比例\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        self.size = d_model\n",
    "        # 自注意力机制\n",
    "        self.self_attn = MultiHeadedAttention(h, d_model)\n",
    "        # 上下文注意力机制\n",
    "        self.src_attn = MultiHeadedAttention(h, d_model)\n",
    "        # 前馈全连接子层\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        # 子层连接结构\n",
    "        self.sublayer1 = SublayerConnection(d_model, dropout)\n",
    "        self.sublayer2 = SublayerConnection(d_model, dropout)\n",
    "        self.sublayer3 = SublayerConnection(d_model, dropout)\n",
    "        # 规范化层\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        # memory为编码器输出隐表示\n",
    "        m = memory\n",
    "        # 自注意力机制，q、k、v均来自解码器隐表示\n",
    "        x = self.sublayer1(x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        # 上下文注意力机制：q为来自解码器隐表示，而k、v为编码器隐表示\n",
    "        x = self.sublayer2(x, lambda x: self.src_attn(x, m, m, src_mask))\n",
    "        return self.norm(self.sublayer3(x, self.feed_forward))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8215038",
   "metadata": {},
   "source": [
    "## 12 生成器类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c09d0aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    #  生成器层\n",
    "    def __init__(self, d_model, vocab):\n",
    "        \"\"\"\n",
    "        生成器层初始化\n",
    "        :param d_model:\n",
    "        :param vocab:\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "        # decode后的结果，先进入一个全连接层变为词典大小的向量\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 然后再进行log_softmax操作(在softmax结果上再做多一次log运算)\n",
    "        return F.log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d318d0eb",
   "metadata": {},
   "source": [
    "## 13 transformer框架"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "00026630",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    # Transformer层\n",
    "    def __init__(self, tokenizer, h=8, d_model=256, E_N=2, D_N=2, device='cuda'):\n",
    "        \"\"\"\n",
    "        transformer层初始化\n",
    "        :param h: 头数\n",
    "        :param d_model: 词嵌入纬度\n",
    "        :param tokenizer:\n",
    "        :param E_N:\n",
    "        :param D_N:\n",
    "        :param device:\n",
    "        \"\"\"\n",
    "        super(Transformer, self).__init__()\n",
    "        # 编码器\n",
    "        self.encoder = nn.ModuleList([Encoder(h, d_model) for _ in range(E_N)])\n",
    "        # 解码器\n",
    "        self.decoder = nn.ModuleList([Decoder(h, d_model) for _ in range(D_N)])\n",
    "        # 词嵌入层\n",
    "        self.src_embed = Embedding(d_model, tokenizer.get_vocab_size())\n",
    "        self.tgt_embed = Embedding(d_model, tokenizer.get_vocab_size())\n",
    "        # 位置编码器层\n",
    "        self.src_pos = PositionalEncoding(d_model, device=device)\n",
    "        self.tgt_pos = PositionalEncoding(d_model, device=device)\n",
    "        # 生成器层\n",
    "        self.generator = Generator(d_model, tokenizer.get_vocab_size())\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        \"\"\"\n",
    "        编码\n",
    "        :param src: 源数据\n",
    "        :param src_mask: 源数据掩码\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        \n",
    "        # 词嵌入\n",
    "        src = self.src_embed(src)\n",
    "        # 位置编码\n",
    "        src = self.src_pos(src)\n",
    "        # 编码\n",
    "        for i in self.encoder:\n",
    "            src = i(src, src_mask)\n",
    "        return src\n",
    "\n",
    "    def decode(self, memory, tgt, src_mask, tgt_mask):\n",
    "        \"\"\"\n",
    "        解码\n",
    "        :param memory: 编码器输出\n",
    "        :param tgt: 目标数据输入\n",
    "        :param src_mask: 源数据掩码\n",
    "        :param tgt_mask: 目标数据掩码\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        #  词嵌入\n",
    "        tgt = self.tgt_embed(tgt)\n",
    "        #  位置编码\n",
    "        tgt = self.tgt_pos(tgt)\n",
    "        # 解码\n",
    "        for i in self.decoder:\n",
    "            tgt = i(tgt, memory, src_mask, tgt_mask)\n",
    "        return tgt\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        \"\"\"\n",
    "        反向传播\n",
    "        :param src: 源数据\n",
    "        :param tgt: 目标数据\n",
    "        :param src_mask: 源数据掩码\n",
    "        :param tgt_mask: 目标数据掩码\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # encoder的结果作为decoder的memory参数传入，进行decode\n",
    "        return self.decode(self.encode(src, src_mask), tgt, src_mask, tgt_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf8b6f2",
   "metadata": {},
   "source": [
    "## 14 标签平滑类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b02006b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    # 标签平滑\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        \"\"\"\n",
    "        初始化\n",
    "        :param size: 目标数据词表大小\n",
    "        :param padding_idx: 目标数据填充字符的索引\n",
    "        :param smoothing: 做平滑的值，为0即不进行平滑\n",
    "        \"\"\"\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        # KL散度，通常用于测量两个概率分布之间的差异\n",
    "        self.criterion = nn.KLDivLoss(reduction='sum')\n",
    "        # 目标数据填充字符的索引\n",
    "        self.padding_idx = padding_idx\n",
    "        # 置信度\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        # 平滑值\n",
    "        self.smoothing = smoothing\n",
    "        # 词表大小\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        \"\"\"\n",
    "        反向传播\n",
    "        :param x: 预测值\n",
    "        :param target: 目标值\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # 判断输出值的第二维传长度是否等于输出词表的大小，这里x的shape为 （batch*seqlength,x.shape(-1)）\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        # 标签平滑填充\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        # 这里的操作是将真实值的位置进行替换,替换成置信度\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        # 将填充的位置的值设置为0\n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "        # 生成填充部分的掩码\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        self.true_dist = true_dist\n",
    "        # 返回KL散度\n",
    "        return self.criterion(x, Variable(true_dist, requires_grad=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5ab2a5",
   "metadata": {},
   "source": [
    "## 15 损失计算类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3c0f66c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLossCompute:\n",
    "    # 计算损失和进行参数反向传播\n",
    "    def __init__(self, generator, criterion, opt=None):\n",
    "        \"\"\"\n",
    "        初始化\n",
    "        :param generator: 生成器，transformer模块中的最后一层，这里将其单独拿出来而不直接放进transformer中的原因是：\n",
    "            预测数据的是时候，我们需要利用之前的结果，但是我们只去最后一个作为本次输出，那么在进行输出时，只对最后一个进行输出，单独拿出来进行输出的线性变换，更灵活\n",
    "        :param criterion: 标签平滑的类\n",
    "        :param opt: 经wormup后的optimizer\n",
    "        \"\"\"\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "        self.opt = opt\n",
    "\n",
    "    def __call__(self, x, y, norm):\n",
    "        \"\"\"\n",
    "        类做函数调用\n",
    "        :param x: 经transformer解码后的结果\n",
    "        :param y: 目标值\n",
    "        :param norm: 本次数据有效的字符数，即，除去padding后的字符数\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # 进行输出\n",
    "        x = self.generator(x)\n",
    "        # 得到KL散度\n",
    "        loss = self.criterion(x.contiguous().view(-1, x.size(-1)),\n",
    "                              y.contiguous().view(-1)) / norm\n",
    "        # 反向椽笔\n",
    "        loss.backward()\n",
    "        if self.opt is not None:\n",
    "            # 参数更新\n",
    "            self.opt.step()\n",
    "            # 优化器梯度置0\n",
    "            self.opt.optimizer.zero_grad()\n",
    "        # 返回损失\n",
    "        return loss.data.item() * norm.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea106b5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d01a5bc1",
   "metadata": {},
   "source": [
    "## 16 Warmup-学习率更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c275e7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoamOpt:\n",
    "    # warmup\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        \"\"\"\n",
    "        初始化\n",
    "        :param model_size: 词嵌入维度\n",
    "        :param factor:\n",
    "        :param warmup:\n",
    "        :param optimizer:\n",
    "        \"\"\"\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "\n",
    "    def step(self):\n",
    "        # 学习率更新\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "    def rate(self, step=None):\n",
    "        # 学习率更新函数\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * (self.model_size ** (-0.5) * min(step ** (-0.5), step * self.warmup ** (-1.5)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04bf5a5",
   "metadata": {},
   "source": [
    "## 评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3a9fb0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_english_sentence(sentence):\n",
    "    # 使用正则表达式检查句子中是否包含英文字母\n",
    "    english_pattern = re.compile(r'[a-zA-Z]')\n",
    "    match = english_pattern.search(sentence)\n",
    "    # True 表示这是英文句子\n",
    "    if match: \n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "# 这个smooth防止句子长度小于4而出现报错\n",
    "smooth = SmoothingFunction().method1\n",
    "def compute_bleu4(tokenizer, random_integers, model, device):\n",
    "    \"\"\"\n",
    "    计算BLEU4\n",
    "    :param tokenizer: tokenizer\n",
    "    :param random_integers: 这个是随机选择的测试集数据的编号\n",
    "    :param model: 模型\n",
    "    :param device: 设备\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # m1,m2存放英文的原数据与模型输出数据\n",
    "    m1, m2 = [], []\n",
    "    # m3,m4存放英文的原数据与模型输出数据\n",
    "    m3, m4 = [], []\n",
    "    model.eval()\n",
    "    # 存放测试数据\n",
    "    da = []\n",
    "    # 将随机选择的测试集数据编号添加到da中\n",
    "    for i in random_integers:\n",
    "        da.append(tokenizer.test[i])\n",
    "    # 对da中的数据进行编码\n",
    "    labels, x, _ = tokenizer.encode_all(da)\n",
    "    with torch.no_grad():\n",
    "        # 预测\n",
    "        y = predict(x, model, tokenizer, device)\n",
    "    # 这个p用于记录y的索引\n",
    "    p = 0\n",
    "    # 用于保存有效的索引\n",
    "    itg = []\n",
    "    # 这里我限制输入数据全部有效，如果有无效的数据，直接放弃本次计算\n",
    "    if len(y) != 10:\n",
    "        return 0\n",
    "    for i in labels:\n",
    "        # 取出有效的索引\n",
    "        itg.append(random_integers[i])\n",
    "    # 将真实数据与预测数据分别放到m1,m2,m3,m4中\n",
    "    for i in itg:\n",
    "        if is_english_sentence(tokenizer.test[i][1]):\n",
    "            m1.append(tokenizer.test[i][1])\n",
    "            m2.append([y[p]])\n",
    "        else:\n",
    "            m3.append(list(jieba.cut(tokenizer.test[i][1])))\n",
    "            m4.append([list(jieba.cut(y[p]))])\n",
    "        p += 1\n",
    "    smooth = SmoothingFunction().method1\n",
    "    # 计算英文的bleu4\n",
    "    b1 = [sacrebleu.sentence_bleu(candidate, refs).score for candidate, refs in zip(m1, m2)]\n",
    "    # 计算中文的bleu4\n",
    "    for i in range(len(m4)):\n",
    "        b2 = sentence_bleu(m4[i], m3[i], weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smooth) * 100\n",
    "        b1.append(b2)\n",
    "#     print(b1)\n",
    "#     print(sum(b1)/len(b1))\n",
    "    return sum(b1)/len(b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb47ba2",
   "metadata": {},
   "source": [
    "## 17 训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c384beae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "英文 0.0\n",
      "英文 0.014628651677174915\n",
      "英文 0.02925730335434983\n",
      "英文 0.043885955031524745\n",
      "英文 0.05851460670869966\n",
      "英文 0.07314325838587457\n",
      "英文 0.08777191006304949\n",
      "英文 0.1024005617402244\n",
      "英文 0.11702921341739932\n",
      "英文 0.13165786509457422\n",
      "英文 0.14628651677174914\n",
      "英文 0.16091516844892406\n",
      "英文 0.17554382012609898\n",
      "英文 0.1901724718032739\n",
      "英文 0.2048011234804488\n",
      "英文 0.2194297751576237\n",
      "英文 0.23405842683479863\n",
      "英文 0.24868707851197355\n",
      "英文 0.26331573018914844\n",
      "英文 0.27794438186632336\n",
      "英文 0.2925730335434983\n",
      "英文 0.3072016852206732\n",
      "英文 0.3218303368978481\n",
      "英文 0.33645898857502304\n",
      "英文 0.35108764025219796\n",
      "英文 0.3657162919293729\n",
      "英文 0.3803449436065478\n",
      "英文 0.3949735952837227\n",
      "英文 0.4096022469608976\n",
      "英文 0.4242308986380725\n",
      "英文 0.4388595503152474\n",
      "英文 0.45348820199242235\n",
      "英文 0.46811685366959727\n",
      "英文 0.4827455053467722\n",
      "英文 0.4973741570239471\n",
      "英文 0.512002808701122\n",
      "英文 0.5266314603782969\n",
      "英文 0.5412601120554719\n",
      "英文 0.5558887637326467\n",
      "英文 0.5705174154098217\n",
      "英文 0.5851460670869966\n",
      "英文 0.5997747187641715\n",
      "英文 0.6144033704413464\n",
      "英文 0.6290320221185214\n",
      "英文 0.6436606737956962\n",
      "英文 0.6582893254728711\n",
      "英文 0.6729179771500461\n",
      "英文 0.687546628827221\n",
      "英文 0.7021752805043959\n",
      "英文 0.7168039321815708\n",
      "英文 0.7314325838587458\n",
      "英文 0.7460612355359206\n",
      "英文 0.7606898872130956\n",
      "英文 0.7753185388902705\n",
      "英文 0.7899471905674454\n",
      "英文 0.8045758422446203\n",
      "英文 0.8192044939217952\n",
      "英文 0.8338331455989701\n",
      "英文 0.848461797276145\n",
      "英文 0.86309044895332\n",
      "英文 0.8777191006304949\n",
      "英文 0.8923477523076698\n",
      "英文 0.9069764039848447\n",
      "英文 0.9216050556620197\n",
      "英文 0.9362337073391945\n",
      "英文 0.9508623590163695\n",
      "英文 0.9654910106935444\n",
      "英文 0.9801196623707192\n",
      "英文 0.9947483140478942\n",
      "中文 0.0\n",
      "中文 0.014628651677174915\n",
      "中文 0.02925730335434983\n",
      "中文 0.043885955031524745\n",
      "中文 0.05851460670869966\n",
      "中文 0.07314325838587457\n",
      "中文 0.08777191006304949\n",
      "中文 0.1024005617402244\n",
      "中文 0.11702921341739932\n",
      "中文 0.13165786509457422\n",
      "中文 0.14628651677174914\n",
      "中文 0.16091516844892406\n",
      "中文 0.17554382012609898\n",
      "中文 0.1901724718032739\n",
      "中文 0.2048011234804488\n",
      "中文 0.2194297751576237\n",
      "中文 0.23405842683479863\n",
      "中文 0.24868707851197355\n",
      "中文 0.26331573018914844\n",
      "中文 0.27794438186632336\n",
      "中文 0.2925730335434983\n",
      "中文 0.3072016852206732\n",
      "中文 0.3218303368978481\n",
      "中文 0.33645898857502304\n",
      "中文 0.35108764025219796\n",
      "中文 0.3657162919293729\n",
      "中文 0.3803449436065478\n",
      "中文 0.3949735952837227\n",
      "中文 0.4096022469608976\n",
      "中文 0.4242308986380725\n",
      "中文 0.4388595503152474\n",
      "中文 0.45348820199242235\n",
      "中文 0.46811685366959727\n",
      "中文 0.4827455053467722\n",
      "中文 0.4973741570239471\n",
      "中文 0.512002808701122\n",
      "中文 0.5266314603782969\n",
      "中文 0.5412601120554719\n",
      "中文 0.5558887637326467\n",
      "中文 0.5705174154098217\n",
      "中文 0.5851460670869966\n",
      "中文 0.5997747187641715\n",
      "中文 0.6144033704413464\n",
      "中文 0.6290320221185214\n",
      "中文 0.6436606737956962\n",
      "中文 0.6582893254728711\n",
      "中文 0.6729179771500461\n",
      "中文 0.687546628827221\n",
      "中文 0.7021752805043959\n",
      "中文 0.7168039321815708\n",
      "中文 0.7314325838587458\n",
      "中文 0.7460612355359206\n",
      "中文 0.7606898872130956\n",
      "中文 0.7753185388902705\n",
      "中文 0.7899471905674454\n",
      "中文 0.8045758422446203\n",
      "中文 0.8192044939217952\n",
      "中文 0.8338331455989701\n",
      "中文 0.848461797276145\n",
      "中文 0.86309044895332\n",
      "中文 0.8777191006304949\n",
      "中文 0.8923477523076698\n",
      "中文 0.9069764039848447\n",
      "中文 0.9216050556620197\n",
      "中文 0.9362337073391945\n",
      "中文 0.9508623590163695\n",
      "中文 0.9654910106935444\n",
      "中文 0.9801196623707192\n",
      "中文 0.9947483140478942\n"
     ]
    }
   ],
   "source": [
    "en_path = r'H:\\datasets\\data\\翻译1\\src.txt'\n",
    "ch_path = r'H:\\datasets\\data\\翻译1\\tgt.txt'\n",
    "tokenizer = Tokenizer(en_path, ch_path, count_min=3)\n",
    "# 训练\n",
    "def train(): \n",
    "    device = 'cuda'\n",
    "    model = Transformer(tokenizer, device=device)\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            # 这里初始化采用的是nn.init.xavier_uniform\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    model = model.to(device)\n",
    "    criteria = LabelSmoothing(tokenizer.get_vocab_size(), tokenizer.word_2_index['<pad>'])\n",
    "    optimizer = NoamOpt(256, 1, 2000,\n",
    "                        torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
    "    lossF = SimpleLossCompute(model.generator, criteria, optimizer)\n",
    "    epochs = 100\n",
    "    model.train()\n",
    "    loss_all = []\n",
    "    print('词表大小', tokenizer.get_vocab_size())\n",
    "    t = time.time()\n",
    "    data_loader = tokenizer.get_dataloader(tokenizer.data_)\n",
    "    random_integers = random.sample(range(len(tokenizer.test)-10), 6)  # 随机选100个句子\n",
    "    batchs=[]\n",
    "    for index, data in enumerate(data_loader):\n",
    "        src, tgt = data\n",
    "        # 处理一个batch\n",
    "        batch = Batch(src, tgt, tokenizer=tokenizer, device=device)\n",
    "        batchs.append(batch)\n",
    "    for epoch in range(epochs):\n",
    "        p=0\n",
    "        for batch in batchs:\n",
    "            out = model(batch.src, batch.trg, batch.src_mask, batch.trg_mask)\n",
    "            out = lossF(out, batch.trg_y, batch.ntokens)\n",
    "            if (p+1) % 1000 == 0:\n",
    "                model.eval()\n",
    "#                 compute_bleu4(tokenizer, random_integers, model, device)\n",
    "                print('epoch', epoch, 'loss', float(out / batch.ntokens))\n",
    "                model.train()\n",
    "                print('time', time.time() - t)\n",
    "                if float(out / batch.ntokens)<2.2:\n",
    "                    random_integers = random.sample(range(len(tokenizer.test)), 100)\n",
    "                    nu=compute_bleu4(tokenizer, random_integers, model, device)\n",
    "                    if nu > 17:\n",
    "                        torch.save(model.state_dict(), f'./model/translation_{epoch}_{p}.pt')\n",
    "                        break\n",
    "                    if nu > 14:\n",
    "                        torch.save(model.state_dict(), f'./model/translation_{epoch}_{p}.pt')\n",
    "\n",
    "            if p%100==0:\n",
    "                print(p/1000)\n",
    "            p+=1\n",
    "        \n",
    "        loss_all.append(float(out / batch.ntokens))\n",
    "        \n",
    "        \n",
    "\n",
    "    with open('loss.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write(str(loss_all))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59aecc9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "298abc2a",
   "metadata": {},
   "source": [
    "## 18 预测函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "af7ad45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import words\n",
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    \"\"\"\n",
    "    传入一个训练好的模型，对指定数据进行预测\n",
    "    \"\"\"\n",
    "    # 先用encoder进行encode\n",
    "    memory = model.encode(src, src_mask)\n",
    "    # 初始化预测内容为1×1的tensor，填入开始符('BOS')的id，并将type设置为输入数据类型(LongTensor)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
    "    # 遍历输出的长度下标\n",
    "    for i in range(max_len - 1):\n",
    "        # decode得到隐层表示\n",
    "        out = model.decode(memory,\n",
    "                           Variable(ys),\n",
    "                           src_mask,\n",
    "                           Variable(subsequent_mask(ys.size(1)).type_as(src.data)))\n",
    "        # 将隐藏表示转为对词典各词的log_softmax概率分布表示\n",
    "        prob = model.generator(out[:, i])\n",
    "        # 获取当前位置最大概率的预测词id\n",
    "        _, next_word = torch.max(prob, dim=-1)\n",
    "        next_word = next_word.data[0]\n",
    "        # 将当前位置预测的字符id与之前的预测内容拼接起来\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
    "    return ys\n",
    "\n",
    "\n",
    "def predict(data, model, tokenizer, device='cuda'):\n",
    "    \"\"\"\n",
    "    在data上用训练好的模型进行预测，打印模型翻译结果\n",
    "    \"\"\"\n",
    "    # 梯度清零\n",
    "    with torch.no_grad():\n",
    "        # 在data的英文数据长度上遍历下标\n",
    "        data1=[]\n",
    "        for i in range(len(data)):\n",
    "            # 打印待翻译的英文语句\n",
    "\n",
    "            # 将当前以单词id表示的英文语句数据转为tensor，并放如DEVICE中\n",
    "            src = torch.from_numpy(np.array(data[i])).long().to(device)\n",
    "            # 增加一维\n",
    "            src = src.unsqueeze(0)\n",
    "            # 设置attention mask\n",
    "            src_mask = (src != tokenizer.word_2_index['<pad>']).unsqueeze(-2)\n",
    "            # 用训练好的模型进行decode预测\n",
    "            out = greedy_decode(model, src, src_mask, max_len=100, start_symbol=tokenizer.word_2_index['<bos>'])\n",
    "            # 初始化一个用于存放模型翻译结果语句单词的列表\n",
    "            translation = []\n",
    "            # 遍历翻译输出字符的下标（注意：开始符\"BOS\"的索引0不遍历）\n",
    "            for j in range(1, out.size(1)):\n",
    "                # 获取当前下标的输出字符\n",
    "                sym = tokenizer.index_2_word[out[0, j].item()]\n",
    "                # 如果输出字符不为'EOS'终止符，则添加到当前语句的翻译结果列表\n",
    "                if sym != '<eos>':\n",
    "                    translation.append(sym)\n",
    "                # 否则终止遍历\n",
    "                else:\n",
    "                    break\n",
    "            # 打印模型翻译输出的中文语句结果\n",
    "            if len(translation)>0:\n",
    "                if translation[0].lower() in words.words():\n",
    "                    data1.append(TreebankWordDetokenizer().detokenize(translation))\n",
    "                else:\n",
    "                    data1.append(\"\".join(translation))\n",
    "        return data1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54989ea",
   "metadata": {},
   "source": [
    "## 19 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2496758e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1b54981e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib qt\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a9a7b4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('loss.txt','r',encoding='utf-8') as f:\n",
    "#     data=f.read()\n",
    "# data=eval(data)\n",
    "# fig=plt.figure()\n",
    "# plt.plot([i*100 for i in range(len(data))],data)\n",
    "# plt.xlabel('batch_num')\n",
    "# plt.ylabel('loss')\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ede3b1",
   "metadata": {},
   "source": [
    "## 20 加载之前的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2b1d09fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# en_path = r'H:\\datasets\\data\\翻译1\\test.en.txt'\n",
    "# ch_path = r'H:\\datasets\\data\\翻译1\\test.ch.txt'\n",
    "# tokenizer = Tokenizer(en_path, ch_path, count_min=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "51ea194b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据评估\n",
    "def eval1():\n",
    "    device='cuda'\n",
    "    model1 = Transformer(tokenizer)\n",
    "    model1.load_state_dict(torch.load(f'./model/translation_25.pt'))\n",
    "    model1 = model1.to(device)\n",
    "    model1.eval()\n",
    "    all_=[]\n",
    "    for i in range(100):\n",
    "        random_integers = range(len(tokenizer.test))[i*10:i*10+10]  # 评估\n",
    "        end=compute_bleu4(tokenizer, random_integers, model1, device)\n",
    "        if end==0:\n",
    "            continue\n",
    "        all_.append(end)\n",
    "    print(sum(all_)/len(all_)) # 输出bleu4得分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dff5b29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.925706747710073\n",
      "12.415454583833347\n",
      "13.398549480576941\n",
      "13.265693156666217\n",
      "12.222975457859016\n",
      "13.084871746486344\n",
      "12.481872803227324\n",
      "13.438320849368786\n",
      "13.575276319146546\n",
      "13.908769162994911\n",
      "14.809956821513763\n",
      "15.3124888267253\n",
      "15.188909659051168\n",
      "15.121003256173358\n"
     ]
    }
   ],
   "source": [
    "eval1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73be28b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
